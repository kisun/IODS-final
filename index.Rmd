---
title: Boston Housing Dataset
author: 
  name: Kisun Pokharel
  affiliation: Natural Resources Institute Finland (Luke)
  email: kisun.pokharel@helsinki.fi
date: December 18, 2017
---  


#Abstract
This report is the final assignment of the [IODS course](https://wiki.helsinki.fi/display/SocStats/Introduction+to+Open+Data+Science%2C+spring+2017). The Boston dataset available from MASS package was used to perform multiple linear regression analysis . The analysis showed that the housing price in Boston may determine if the owner could have crime history. In other words, the price of owner occupied homes proved to be highly significant in determining the crime rate.


#Introduction  
This mini-project focuses on predicting the crime rate in Boston using the Boston housing dataset that is available via MASS R package. The aim of the project is to construct a mathematical model using multiple regression and predict the crime rate based on a set of predictor variables. In multiple regression approach, we make prediction on a dependent variable (here *crime*) by using more than one dependent variables.

#Data Wrangling  
Before proceeding with the statistical analyses, we will first go through some data wrangling steps. In many cases, the available (raw) data may contain noise such as missing values and are usually untidy and poorly formatted. Therefore, the aim of data wrangling step is to convert raw data into clean (analysis ready) data and to make sure that the dataset satisfies the following conditions: 

1. all observations are stored in rows
2. all variables are in columns
3. all data under study are kept in single dataset


The R script for data wrangling can be found with [this link](https://github.com/kisun/IODS-final/blob/master/data/create_boston.R).


#Data Summary  
The **Boston** data was collected to study the housing values in the suburbs of Boston. The table contains 506 observations for 14 different variables. The descriptions for each of the 14 variables are listed below.

Variables | Description
--------- | -----------------------------------------------------------------
crim | per capita crime rate by town.
zn | proportion of residential land zoned for lots over 25,000 sq.ft.
indus | proportion of non-retail business acres per town.
chas | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
nox | nitrogen oxides concentration (parts per 10 million).
rm | average number of rooms per dwelling.
age | proportion of owner-occupied units built prior to 1940.
dis | weighted mean of distances to five Boston employment centres.
rad | index of accessibility to radial highways.
tax | full-value property-tax rate per \$10,000.
ptratio | pupil-teacher ratio by town.
black | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
lstat | lower status of the population (percent).
medv | median value of owner-occupied homes in \$1000s.


First and foremost, it is important to get an overview of the data being analysed. As mentioned earlier, Boston data is available from [MASS package](https://cran.r-project.org/web/packages/MASS/index.html) and thus can be directly imported into R for further analysis.

```{r, libraries, message=FALSE, echo=FALSE, warning=FALSE}
library(MASS)
library(pander)
library(corrplot)
library(dplyr)
library(ggplot2)
library(GGally)

```

```{r, ex2, message=FALSE, echo=FALSE, warning=FALSE}
#load MASS package and read the Boston data

data(Boston)
```

#Data Exploration
##Preliminary hypotheses
Just by looking at the varibles above, one can make few assumptions regarding the inter-relationships between different variables. Let's start with some assumptions:

Here, the main goal of the analysis is to study how crime rate is associated with other aspects in Boston. After going through the background information, it is a bit easier to identify interesting variables that could be related to criminal activities. Personally, I believe that the following are the four interesting variables that are associated with crime rates:  

**1. Accessibility to radial highways (rad)** : In my opinion, accessibility to radial highways plays vital role when it comes to crime related activities. Criminals may prefer towns with rapid access to radial highways especially for escaping purpose. Therefore, I believe that accessibility to radial highways may contribute to higher crime rates.  

**2. Value of owner-occupied homes (medv)** : Usually, the crime-related activities are common with individuals who do not have enough property. They say that one of the main reasons for crime is poverty. Individual with basic income may also have homes with lower value but I think we can also correlate the value of house one has with crime-rate. In other words, value of owner-occupied home should be negatively correlated with crime.

**3. Distance to employment centers in Boston (dis)** : I hypothesize that the farther the residential area from employment centers, the lower the crime rates. 

**4. Proportion of blacks(black)** : There have been an impression that blacks commit more crime compared to whites. Therefore, I hypothesize that proportion of blacks is positively correlated with crime rate.



In the following section, we will see in details how my hypotheses are explained by the data.


##Non-graphical data exploration
Non-graphical data exploration is the first step before we start analyzing the data. This step different aspects of the data here we will cover three of those:

1. measures of central tendency (mean, meadian)
2. measures of variability (interquartile range, standard deviation)
3. relationships between variables (correlation)

Now, let's first look at the summary of the boston data in the form of table (instead of default layout) using *pandoc.table* function of **pander** package. 

```{r sum1}
pandoc.table(summary(Boston), caption = "Summary of Boston data", split.table = 100)
```
After getting a statistical summary of, it's worthwhile to see to what extent each variables are correlated. For that, we use **corr()** function on Boston data. 

```{r, ex3.2}
corr_boston<-cor(Boston) %>% round(2)
pandoc.table(corr_boston, split.table = 100)
```

The table above shows the correlation matrix of all variables. Bird's eye view on the matrix shows that **tax** (full-value property-tax rate) and **rad** (index of accessibility to radial highways) are the most positively correlated variables, whereas **dis** (weighted mean of distances to five Boston employment centres) and **age** (proportion of owner-occupied units built prior to 1940) are the most negatively correlated variables. Moreover, **chas** (Charles river dummy variable) and **rad** are the two variables that are least correlated.

## Graphical data exploration{.tabset .tabset-pills}

The same information can be presented as a graphical overview. This time we will make a correlogram, a graphical representation of coorelation matrix. We will visualize the summary of the data using box plot and histogram. The *corrplot()* function of **corrplot** package wll be used to visualize the correlation between all the variables of the Boston dataset.


### Correlation plot
```{r, fig.width=9, fig.height=9}
corrplot(corr_boston, method = "circle", tl.col = "black", cl.pos="b", tl.pos = "d", type = "upper" , tl.cex = 0.9 )
```

### Correlation matrix
```{r, ggpairs, fig.width=10, fig.height=10}

ggpairs(Boston)
```


The above graphs gives much quicker impression regarding the variables. In the correlation graph, for example, positive correlations are displayed in blue and negative correlations in red color with intensity of the color and circle size being proportional to the correlation coefficients. The same relationship as described above using correlation summary can be seen in the form of circles with different size (intensity of correlation i.e highly correlated or lowly correlated) and different colors (wheether positively or negatively correlated).

#Data Analysis
After getting an overview of the data, we will employ multiple linear regression model (method) on  the subset of the data to check how four variables (rad, medv, dis  and black) are affecting crim. 


###Multiple Linear Regression 
Multiple linear regression is commonly used method for most of the the regression analyses.  As a predictive analysis method, multiple linear regression describes given data and explain the relationship between one dependent variable and two or more independent variables. Our research question for the multiple linear regression is as follows:
Can we explain the crime rate in Boston area based on the four variables that were hypothesized earlier in this report?

First, we need to check whether there is a linear relationship between the independent variables and the dependent variable in our multiple linear regression model. After that, we fit the model simply using *lm()* function and call output usig *summary()* function.

```{r, plot_myar}
#let's first create subset of Boston data (my_Bos) using five variables kept in (my_var)
myvar<-c("crim", "black", "medv", "dis", "rad")
my_Bos<-select(Boston, one_of(myvar))
ggpairs(my_Bos)
```

Here, the *crime* variable is positively correlated with *rad* (0.626) and negatively correlated with *black* (-0.385), *medv* (-0.388) and *dis*.(-0.38)


```{r, model}
#fit linear model
lm_crim<-lm(crim~. , data=my_Bos)
#summarize the model
summary(lm_crim)
```
From the output, residuals represent the difference between the actual values i.e the crime rate (dependent variable) in Boston and each of the four explanatory variables. The next section shows the coefficients of the model. From the output, we saw that the intercept is 6.529163 with coefficients for dependant variables being -0.009053(black), -0.141722(medv), -0.292778(dis) and 0.483580(rad). Next, we can also see Standard errors for the coefficients which measure the average amount that the coefficient estimates differ from the dependent variables. Moreover, the t-value measures how far the coefficient estimation is away from zero with the assumption that the farther the better. In other words, we reject the null hypothesis when it's far from zero. 

In the above model, the t-values are farthest in *rad* followed by *medv*, *black* and *dis*. These values are also larger compared to standard error except *dis*, which indiciate that these variables do have some relationship with the *crime*. The last column indicates the probability for getting any value >= *t*. In general, a p-value of 0.05 is good cut-off for significance. Thus, we can see that three (*black*, *medv* and *rad*) out of four choosen variables were significant.   

The residual standard error indicates the quality of the regression model. This value some how gives how much we can be confident of our model. Here, the residual error is based on 501 degrees of freedom, where the degrees of freedom are data points used for estimating the parameters. 

Finally, I would like to highlight to what extent my assumptions have been addressed by the model. Although I hypothesized that all of my four chosen variables have impact on crime, it turned out that the distance to employment centers (*dis*) is not significant variable to predict crime rate. Besides that, all three variables were significant with *rad* being the most significant one followed by *medv* and *black*.


###Model validation{.tabset .tabset-pills}

#### Residual vs Fitted
```{r, plot1}
plot(lm_crim, which = 1)
```

#### Normal Q-Q
```{r, plot2}
plot(lm_crim, which = 2)
```

#### Residual vs Leverage
```{r, plot3}
plot(lm_crim, which = 5)
```

###Interpretation (model validation)

The three different diagnostic plots are generated above.The assumptions behind all three models is linearity and normality. Based on the above plots, we can conclude that the errors are normally distributed (clearly observed in q-q plot). Similarly, residual versus fitted model showed that the errors are not dependent on the crime variable. Moreover, we can see that few outliers (381, 406 and 410) have influences to the assumption that is clearly represented in case of residual vs leverage model and can be seen in other two plots as well. All the models have adressed the outliers nicely. Thus, assumptions in all models are more or less valid.


#Conclusion  
To conclude, three of the four variables I selected by assuming that they predict crime rate in Boston were found to be valid. Moreover, it was found that the median values of owner occupied homes turned out to be the best predictor among other variables. 


##Session information:

```{r}
sessionInfo()
```


[![alt text][1.1]][1]
[![alt text][2.1]][2]
[![alt text][3.1]][3]


[1.1]: http://i.imgur.com/tXSoThF.png (Twitter)
[2.1]: http://i.imgur.com/P3YfQoD.png (Facebook)
[3.1]: http://i.imgur.com/0o48UoR.png (Github)

[1]: http://www.twitter.com/kisunpokharel
[2]: http://www.facebook.com/pokharel.kisun
[3]: http://www.github.com/kisun
