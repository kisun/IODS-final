---
title: Boston Housing Dataset
author: 
  name: Kisun Pokharel
  affiliation: Natural Resources Institute Finland (Luke)
  email: kisun.pokharel@helsinki.fi
date: December 18, 2017
---  

<br>

#Abstract
This report is the final assignment of the [IODS course](https://wiki.helsinki.fi/display/SocStats/Introduction+to+Open+Data+Science%2C+spring+2017). The Boston dataset available from MASS package was used to perform xxx . The analysis showed that xx xx and xxx. 


#Introduction  
The Boston Housing Dataset contains information related to housing in the area of Boston Mass.

#Data Wrangling  
Before proceeding with the statistical analyses, we will first go through some data wrangling steps. In many cases, the available (raw) data may contain noise such as missing values and are usually untidy and poorly formatted. Therefore, the aim of data wrangling step is to convert raw data into clean (analysis ready) data and to make sure that the dataset satisfies the following conditions: 

1. all observations are stored in rows
2. all variables are in columns
3. all data under study are kept in single dataset

more

The R script for data wrangling can be found with [this link](https://github.com/kisun/IODS-final/blob/master/data/create_boston.R).


#Data Summary  
The **Boston** data was collected to study the housing values in the suburbs of Boston. The table contains 506 observations for 14 different variables. The descriptions for each of the 14 variables are listed below.

Variables | Description
--------- | -----------------------------------------------------------------
crim | per capita crime rate by town.
zn | proportion of residential land zoned for lots over 25,000 sq.ft.
indus | proportion of non-retail business acres per town.
chas | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
nox | nitrogen oxides concentration (parts per 10 million).
rm | average number of rooms per dwelling.
age | proportion of owner-occupied units built prior to 1940.
dis | weighted mean of distances to five Boston employment centres.
rad | index of accessibility to radial highways.
tax | full-value property-tax rate per \$10,000.
ptratio | pupil-teacher ratio by town.
black | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
lstat | lower status of the population (percent).
medv | median value of owner-occupied homes in \$1000s.


First and foremost, it is important to get an overview of the data being analysed. As mentioned earlier, Boston data from [MASS package](https://cran.r-project.org/web/packages/MASS/index.html). 
```{r ex2}
library(MASS)
data(Boston)
```

#Data Exploration
##Preliminary hypotheses
Just by looking at the varibles, one can make few assumptions regarding the inter-relationships between different variables. Let's do some speculation:

1. 
2. 
3. 
4. 

##Non-graphical data exploration
Now, let's look at the summary of the boston data in the form of table (instead of default layout) using *pandoc.table* function of **pander** package. 
```{r sum1}
library(pander)
pandoc.table(summary(Boston), caption = "Summary of Boston data", split.table = 100)
```
After getting a statistical summary of, it's worthwhile to see to what extent each variables are correlated. For that, we use **corr()** function on Boston data. 

```{r, ex3.2}
library(corrplot)
library(dplyr)
corr_boston<-cor(Boston) %>% round(2)
pandoc.table(corr_boston, split.table = 100)
```

The table above shows the correlation matrix of all variables. Bird's eye view on the matrix shows that **tax** (full-value property-tax rate) and **rad** (index of accessibility to radial highways) are the most positively correlated variables, whereas **dis** (weighted mean of distances to five Boston employment centres) and **age** (proportion of owner-occupied units built prior to 1940) are the most negatively correlated variables. Moreover, **chas** (Charles river dummy variable) and **rad** are the two variables that are least correlated.

##Graphical data exploration

The same information can be presented as a graphical overview. This time we will make a correlogram, a graphical representation of coorelation matrix. The *corrplot()* function of **corrplot** package wll be used to visualize the correlation between all the variables of the Boston dataset.
```{r, fig.width=9, fig.height=9}
corrplot(corr_boston, method = "circle", tl.col = "black", cl.pos="b", tl.pos = "d", type = "upper" , tl.cex = 0.9 )
```

The above graph gives much quicker impression on which variables are more correlated to each other. In the graph, positive correlations are displayed in blue and negative correlations in red color with intensity of the color and circle size being proportional to the correlation coefficients. The same relationship as described above using correlation summary can be seen in the form of circles with different size (intensity of correlation i.e highly correlated or lowly correlated) and different colors (wheether positively or negatively correlated).

#Data Analysis  
##Data Standardization

Data scaling is useful for linear discriminant analysis. The **scale()** function will be used to scale the whole data. Here, the scaled value is generated by subtracting the column means from corresponding columns and then the difference is divided by standard deviation. i.e scaled(x)=(x-mean(x))/sd(x). 
```{r ex_4.1}
boston_scaled<-scale(Boston)
pandoc.table(summary(boston_scaled), caption = "Summary of  Scaled Boston data", split.table = 120)
#corr_bostons<-cor(boston_scaled) %>% round(2)
#pandoc.table(corr_bostons, split.table = 120)
```

We can make important observations on the summary of scaled data. The summary of the scaled Boston data has changed from the non-scaled Boston data. Most importantly, all the mean values have become zero and other values such as minimum, maximum, median and quartiles (1st and 3rd) are also changed for all variables. 

Next, we will create quantile vector for crime using **quantile** function on scaled dataframe of Boston dataset. The quantile vectors will be labeled with meaningful labels to explain the intensity of crime i.e low, medium low, medium high and high. Lastly, we will replace the **Crim** variable with newly created **crime** variable and create the required data frame.

```{r, ex_4.2}
boston_scaled<- data.frame(boston_scaled)
qvc<-quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = qvc, label = c("low", "med_low", "med_high", "high"), include.lowest = TRUE)
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled<-data.frame(boston_scaled, crime)
#table(boston_scaled$crime)
```

##Linear Regression  


##Logistic Regression  

##PCA


#Conclusion  

what is goin on?

##Session information:

```{r}
sessionInfo()
```

more icons: http://www.entypo.com/index.php

[![alt text][1.1]][1]
[![alt text][2.1]][2]
[![alt text][3.1]][3]


[1.1]: http://i.imgur.com/tXSoThF.png (Twitter)
[2.1]: http://i.imgur.com/P3YfQoD.png (Facebook)
[3.1]: http://i.imgur.com/0o48UoR.png (Github)

[1]: http://www.twitter.com/kisunpokharel
[2]: http://www.facebook.com/pokharel.kisun
[3]: http://www.github.com/kisun
